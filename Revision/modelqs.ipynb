{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an example of a computational problem for which you would prefer (BFS)(DFS) and v.v\n",
    "\n",
    ">BFS could be preferred is finding the shortest path between two nodes in an unweighted graph. \n",
    ">Note if DFS was used for such a problem, it could also find a path from a start node to a desired destination note, however one would not know if it is the shortest possible path.\n",
    ">    \n",
    ">For DFS is finding a path between two nodes in a maze or labyrinth with obstacles. find a solution which is not necessarily the most optimal one.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it said that Floyd-Warshall algorithm for all-pairs shortest path computation is a dynamic programming algorithm? What makes Dijkstra's algorithm a greedy algorithm? Justify your answers. \n",
    "\n",
    "> Floyd-Warshall is dynamic programming algorithm as it uses the principle of optimal substructure to build up a solution to a larger problem using solutions to subproblems. This algorithm computes all-pairs shortest path in a weighted graph by considering every pair of vertices, and computing the shortest path between them. It relies on the following property:\n",
    "> * a shortest path, $d^{k}_{i,j}$, from vertex, $i$, to vertex, $j$, made only of intermediate vertices $\\{1,\\cdots, k\\}$ is the minimum of: \n",
    "> * (1) some path $i\\leadsto k \\leadsto j$ and \n",
    "> * (2) a shortest path,$d^{k-1}_{i,j}$, from $i$ to $j$ which does not go through vertex, $k$, and only uses intermediate vertices in $\\{1,\\cdots,k-1\\}$. <br><br>\n",
    ">\n",
    ">Dijkstra's algorithm can be classified as a greedy algorithm because it selects the shortest path from a source vertex to all other vertices by making the locally optimal choice. It starts by selecting the vertex with the smallest distance from the source vertex and then iteratively builds the shortest path tree by selecting the next vertex with the smallest distance to the so far computed shortest path tree. This process continues until all vertices have been added to the shortest path tree."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the differences (if any) in the run time performance and/or memory requirements between a recursive solution with memoization and a dynamic programming solution. In your answer use an example of a concrete algorithm.\n",
    "\n",
    ">Memoization technique allows to improve the performance of recursive algorithms. When using memoization, the solutions to subproblems solved by the recursive algorithm are stored in an array, and are loaded from such array instead of performing a recursive call if they had been computed before. This improves performance over standard recursive solution as solutions to subproblems are never computed twice.\n",
    ">    \n",
    ">In a typical bottom-up dynamic programming approach, solutions to problems are constructed from solutions to smaller subproblems stored in a table. This approach is slightly less intuitive than memoisation, however it has less overhead as it does not require recursive calls of functions and is usually implemented via nested for loops. \n",
    ">    \n",
    ">If a solution to a problem with some particular input requires one to examine a sparse set of subproblems (e.g. solving 0-1 knapsack problem where all weights are integers divisible by 5) then the memoized solution may be significantly more efficient both in terms of memory usage and run time than a straight-forward bottom up dynamic programming implementation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree's"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how the inorder successor of a node is found in a Binary Search Tree. What is the average case and worst case asymptotic complexity of this operation?\n",
    "\n",
    ">If the current node has a right subtree, then its inorder successor is the maximum key in this subtree. Otherwise, the successor is found by moving up the tree from the current node until a (child-parent) link is found that \"points right\" (i.e. the key of the parent is greater than the child). At this point, the search stops and the parent is returned as the successor.\n",
    ">\n",
    ">The complexity of this operation is $O(\\log n)$ on average case inputs, but $O(n)$ on worst case inputs (in which the tree is a linear chain).\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Input array $A$ of $n$ keys that has been generated by a stochastic process of the form:\n",
    "$$A(i) = i + \\mu$$\n",
    "where $\\mu$ is random noise drawn from the discrete uniform distribution, $\\mu \\sim \\mathrm{unif}\\{-k, k\\}$, for some $k$, where $k \\ll n$. \n",
    "\n",
    "Implement an in-place variant of the quicksort that should have a worst-case run time of $O(n \\log n)$.\n",
    "\n",
    "> swapping the middle index and the high index values before performing the partition so that the pivot becomes the middle index element\n",
    ">\n",
    "> Since $k < < n$, for most of the recursive calls the split will be sufficiently balanced as the middle element can only be larger than its index by the value of +k and the highest index element can only be smaller than its index by the value of -k. Note, from lecture notes even if seemingly unbalanced splits of 0.995n (left) and 0.005 (right) are used the run time of Quicksort is $O(n\\log n)$. \n",
    "\n",
    "What is the expected run time of the quicksort algorithm if the rank of the pivot selected during each recursion does not depend on the input array size? \n",
    "\n",
    "> The expected run time is $O(n^2)$.  This can be seen by noting that since quicksort is recursive:\n",
    "> $T(n) = T(r - 1) + T(n - r) + \\Theta(n)$\n",
    "> Where the partition is $\\Theta(n)$ and $r$ is the rank of the pivot.<br><br>\n",
    "> \n",
    "> If $r$ is some constant $C$ that is independent of $n$, then this expression becomes:\n",
    "> $T(n) = T(C) + T(n - C) + \\Theta(n)$  <br>\n",
    "> which has solution $\\Theta(n^2)$. \n",
    "\n",
    "What is the expected run time of the quicksort algorithm if the rank of the pivot selected during each recursion is $K \\cdot n$ for some $K$ such that $0 < K < 1$.\n",
    "\n",
    ">The expected run time is $O(n\\log n)$. \n",
    ">$T(n) = T(r - 1) + T(n - r) + \\Theta(n)$.\n",
    ">\n",
    ">This time, we will produce a recursion tree of depth $\\log_{\\frac{1}{K}} n$, and so the >overall complexity is $O(n \\log n)$ (we ignore the change of base in the log since it >falls out as a constant factor).\n",
    "\n",
    "To guarantee o n log n:\n",
    "\n",
    "> Select the pivot using the $O(n)$ \"median of medians\" algorithm \n",
    "> This guarantees that we will get a \"sufficiently good\" median. \n",
    "> the overhead of this algorithm tends to be significantly greater than simpler median selection approaches (such as random selection).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
