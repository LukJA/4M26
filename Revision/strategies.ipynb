{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amortized Cost** \n",
    "\n",
    "For an operation sequence, a direct application of worst case analysis per operation may be a pesimistic evaluation of the performance.\n",
    "\n",
    "* Average the running times of operations in a sequence over that sequence.\n",
    "* For a given operation (e.g. <b>multipop</b>), the cost can be much smaller than expected from an individual, per operation analysis. \n",
    "* In a stack, since each element can be pushed onto stack and poped out of the stack at most once, the actual total worst case run time cost of a sequence of $n$ operations is $O(n)$.\n",
    "    * leading to $O(1)$ amortized cost per operation. \n",
    "* While stack with multipop can have some seemingly inefficient operations such as <b>multipop</b>, considering its performance over a sequence operations, reveals that such data structure is indeed highly efficient.\n",
    "    \n",
    "Note that amortized cost analysis is not required in cases when performing operations in a sequence does not change their cost compared to isolated execution of individual operations (e.g. a stack with only <b>push</b> and <b>pop</b> operations). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Design Strategies\n",
    "\n",
    "| Approach | Description |\n",
    "| ----------- | ----------- |\n",
    "| Divide & Conquer | Breaks problem into subproblems like original but smaller, solve recursively and combine |\n",
    "| Dynamic Programming | Applies like _divide & conquer_ but when subproblems overlap making combination more efficient |\n",
    "| Greedy Algorithms | Problem is divided in such as way that combination is performed in a locally optimal manner |\n",
    "| Others | Brute-Force, Backtracking, Branch & Bound, Transform & Conquer, etc. |\n",
    "\n",
    "### Divide & Conquer\n",
    "\n",
    "Breaks problem into **subproblems** like original but smaller, solve recursively and combine.\n",
    "\n",
    "RTA solvable by recursion relationships.\n",
    "\n",
    "- **Divide** the problem into one or more subproblems that are smaller instances of the\n",
    "same problem.\n",
    "- **Conquer** the subproblems by solving them recursively.\n",
    "- **Combine** the subproblem solutions to form a solution to the original problem.\n",
    "\n",
    "Examples: \n",
    "- _Sorting_ - **MergeSort, QuickSort, Heapsort**\n",
    "- _Search_ - Binary Search\n",
    "- _Computation_ - Strassen's Matrix Multiplication\n",
    "- _Signal Processing_ - FFT\n",
    "- _Geometric Algorithms_ - Closest Pair\n",
    "\n",
    "**MergeSort**\n",
    "Divide the sequence into to two subsequences, and recurse merge sort over them\n",
    "- A single item is always sorted\n",
    "- The merge operation via an iterated insert is  at the highest level $\\Theta(n)$\n",
    "\n",
    "\n",
    "### Dynamic Programming\n",
    "\n",
    "Problems exhibit **optimal sub-structure** and **overlapping subproblems**.\n",
    "\n",
    "Optimal sub-structure is when optimal solutions to a problem incorporate optimal solutions to related subproblems, which you may solve independently. Dynamic programming builds an optimal solution to the problem from optimal  solutions to subproblems.\n",
    "\n",
    "Usually includes a form of memory of subproblems from which others are built.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Characterize the structure of an optimal solution\n",
    "2. Recursively define the value of an optimal solution\n",
    "3. Compute the value of an optimal solution (bottom up)\n",
    "4. Construct an optimal solution from computed information\n",
    "\n",
    "Examples: Bellman equation, Bellman-Ford, **0-1 Knapsack Problem**, Longest common subsequence\n",
    "\n",
    "### Greedy Algorithms\n",
    "\n",
    "Greedy algorithms are a subclass of dynamic programming where the locally optimal solution of a subproblem gives a globally optimal solution, i.e. we can make very efficient and optimal algorithms by only considering local state.\n",
    "\n",
    "These are simple to find and to use, but sometime hard to prove.\n",
    "\n",
    "Examples\n",
    "- _Graph Algorithms_ - Kruskal, Primm, Dijkstra \n",
    "- _Optimisation_ - Fractional Knapsack\n",
    "- _Scheduling_ - **Activity Selection Problem**\n",
    "\n",
    "### Recursive Memoization\n",
    "\n",
    "Memoization improves the performance of **recursive algorithms**. \n",
    "\n",
    "When using memoization:\n",
    "* Solutions to subproblems are stored in an array\n",
    "* They are loaded from such array instead of performing a recursive call if they had been computed before. \n",
    "* This improves performance over standard recursive solution as solutions to subproblems are never computed twice.\n",
    "\n",
    "Vs Dynamic Programming:\n",
    "* In typical bottom-up dynamic programming solutions to problems are constructed from solutions to smaller subproblems stored in the table. \n",
    "* This approach is slightly less intuitive than memoisation\n",
    "    * It has less overhead as it does not require recursive calls of functions and is usually implemented via nested loops. \n",
    "* If a solution to a particular input of the problem requires one to examine a sparse set of subproblems then the memoized solution may be significantly more efficient both in terms of memory usage and run time than a straight-forward bottom up dynamic programming implementation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
